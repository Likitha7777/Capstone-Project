# -*- coding: utf-8 -*-
"""cappii (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P0gFXtp84ivDgDDo7XlJscIdmwbQdUEf
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html /content/cappii.ipynb

"""**Installing Virtual Environment**"""

!pip3 install virtualenv
!virtualenv theanoEnv

"""**Activating the Environment**"""

!source /content/theanoEnv/bin/activate; pip3 install theano

!source /content/theanoEnv/bin/activate; pip3 list

!source /content/theanoEnv/bin/activate; pip3 install robotframework; pip3 list; python3 -m robot --help

"""**Installing Streamlit for web app**"""

pip install streamlit

pip install --upgrade streamlit

"""**Mounting Google Drive to Access the dataset**"""

from google.colab import drive
drive.mount('/content/drive')

"""**Installing pyngrok to run web application from colab**"""

pip install pyngrok

"""**Using the authorization token from ngrok account**"""

from pyngrok import ngrok
!ngrok authtoken 2HzT299FPnVroFdHbyvlPfjcLLn_2dHN6JAJDL7HMzCWAzBcc

public_url = ngrok.connect(port='8501')
public_url

"""**Installing Data Cleaning Library**"""

pip install text_hammer

!pip install spacy
!python -m spacy download en_core_web_sm
!pip install beautifulsoup4
!pip install textblob

pip install transformers

import nltk
nltk.download('stopwords')

"""**Loading Data, Data Cleaning, and some Data Visualizations, Accuracy**




"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/Visualize_The_Data.py
# import streamlit as st 
# import pandas as pd
# import text_hammer as th
# import re
# from wordcloud import STOPWORDS
# from nltk.corpus import stopwords
# from wordcloud import WordCloud
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# from tensorflow.keras.layers import Input, Dense
# from transformers import AutoTokenizer,TFBertModel
# max_len = 36
# import tensorflow as tf
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy
# from tensorflow.keras.metrics import CategoricalAccuracy,BinaryAccuracy
# 
# 
# train_data = pd.read_csv('/content/drive/MyDrive/nlp-getting-started (2)/train.csv',usecols=['id','text','target'])
# test_data = pd.read_csv('/content/drive/MyDrive/nlp-getting-started (2)/test.csv',usecols=['id','text'])
# sample_data = pd.read_csv('/content/drive/MyDrive/nlp-getting-started (2)/sample_submission.csv')
# st.title("Natural Language Processinig with Disaster Tweets")
# tab1, tab2, tab3, tab4, tab5, tab6, tab7  = st.tabs(["Train Data", "Test Data", "Cleaned Data", "Data Visualizations - 1","Data Visualizations - 2","Data Visualizations -3", "Model Accuracy"])
# 
# with tab1:
#   st.dataframe(train_data)
#   st.markdown('**Number of Rows and Columns in Train Data:**')
#   train_data.shape
# with tab2:
#   st.dataframe(test_data)
#   st.markdown('**Number of Rows and Columns in Test Data:**')
#   test_data.shape
# with tab3:
#   #Converting to lower case and replacing _ with empty string
#   train_data['text'] = train_data['text'].apply(lambda x:str(x).lower().replace('_', ' '))
#   #Removing emails
#   train_data['text'] = train_data['text'].apply(lambda x: th.remove_emails(x))
#   #Removing HTML Tags
#   train_data['text']=train_data['text'].apply(lambda x: th.remove_html_tags(x))
#   #Removing Urls
#   train_data['text']=train_data['text'].apply(lambda x: th.remove_urls(x))
#   #Removing Special Characters
#   train_data['text'] = train_data['text'].apply(lambda x: th.remove_special_chars(x))
#   #Removing Accented Characters
#   train_data['text'] = train_data['text'].apply(lambda x: th.remove_accented_chars(x))
#   st.markdown('**Cleaned Data:**')
#   st.dataframe(train_data)
# with tab4:
#   stop_words = set(stopwords.words('english'))
#   train_data['text'] = train_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
#   #Disaster Tweets wordcloud 
#   disaster_tweets = train_data[train_data.target == 1]
#   disaster_string = []
#   for t in disaster_tweets.text:
#     disaster_string.append(t)
#   disaster_string = pd.Series(disaster_string).str.cat(sep=' ')
#   wordcloud = WordCloud(width=1600, height=800,max_font_size=100, background_color='white').generate(disaster_string)
#   plt.figure(figsize=(12,10))
#   plt.imshow(wordcloud, interpolation="bilinear")
#   plt.axis("off")
#   st.set_option('deprecation.showPyplotGlobalUse', False)
#   st.markdown("**Disaster Tweets Word Cloud**")
#   st.pyplot()
#   formal_tweets = train_data[train_data.target == 0]
#   formal_string = []
#   for t in formal_tweets.text:
#       formal_string.append(t)
#   formal_string = pd.Series(formal_string).str.cat(sep=' ')
#   wordcloud = WordCloud(width=1600, height=800,max_font_size=100, background_color='white').generate(formal_string)
#   plt.figure(figsize=(12,10))
#   plt.imshow(wordcloud, interpolation="bilinear")
#   plt.axis("off")
#   plt.show()
#   st.set_option('deprecation.showPyplotGlobalUse', False)
#   st.markdown("**Positive Tweets Word Cloud**")
#   st.pyplot()
# with tab5:
#     fig, ax = plt.subplots(1, 2, figsize=(12, 4))
#     train_data["target"].value_counts().plot(
#     kind="pie", 
#     explode=[0.05 for x in train_data["target"].unique()], 
#     autopct='%.2f%%',
#     ax=ax[0], 
#     shadow=True)
#     ax[0].set_title(f"Target Distribution Pie Chart")
#     ax[0].set_ylabel('')
#     #seaborn.countplot() method is used to Show the counts of observations in each categorical bin using bars.
#     count = sns.countplot(x="target", data=train_data, ax=ax[1])
#     for bar in count.patches:
#         count.annotate(format(bar.get_height()),
#             (bar.get_x() + bar.get_width() / 2,
#             bar.get_height()), ha='center', va='center',
#             size=11, xytext=(0, 5),
#             textcoords='offset points')
#     ax[1].set_title(f"Target Distribution Bar Chart")
#     plt.show()
#     st.markdown('**Positive and Disaster Tweets Distribution in Train Dataset**')
#     st.pyplot()
#     st.markdown('**Number of Characters in each Tweet (Train Data)**')
#     train_data["No. of Characters"] = train_data["text"].apply(lambda x: len(x))
#     st.dataframe(train_data.sort_values(by="No. of Characters", ascending=False))
#     train_data["No. of Words"] = train_data["text"].apply(lambda x: len(str(x).split()))
#     st.markdown('**Number of Words in each Tweet (Train Data)**')
#     st.dataframe(train_data.sort_values(by="No. of Words", ascending=False))
# with tab6:
#   fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
#   word=train_data[train_data['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])
#   sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')
#   ax1.set_title('disaster')
#   word=train_data[train_data['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])
#   sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')
#   ax2.set_title('Not disaster')
#   fig.suptitle('Average word length in each tweet')
#   st.markdown('**Average word length in a Tweet (Train Data)**')
#   st.pyplot()
# with tab7:
#   #Loading the BERT Model
#   tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')
#   bert = TFBertModel.from_pretrained('bert-large-uncased')
#   #Conversion of our text data into BERT input format
#   print("max len of tweets",max([len(x.split()) for x in train_data.text]))
#   max_length = 36
#   train_x = tokenizer(
#       text=train_data.text.tolist(),
#       add_special_tokens=True,
#       max_length=36,
#       truncation=True,
#       padding=True, 
#       return_tensors='tf',
#       return_token_type_ids = False,
#       return_attention_mask = True,
#       verbose = True)
#   st.markdown('**Tensor Shape of Input ids**')
#   st.write(train_x['input_ids'].shape)
#   st.markdown('**Tensor Shape of Attention Mask**')
#   st.write(train_x['attention_mask'].shape)
#   train_y = train_data.target.values
#   input_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_ids")
#   input_mask = Input(shape=(max_len,), dtype=tf.int32, name="attention_mask")
#   embeddings = bert(input_ids,attention_mask = input_mask)[1]
#   out = tf.keras.layers.Dropout(0.1)(embeddings)
#   out = Dense(128, activation='relu')(out)
#   out = tf.keras.layers.Dropout(0.1)(out)
#   out = Dense(32,activation = 'relu')(out)
#   y = Dense(1,activation = 'sigmoid')(out)
#   model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)
#   model.layers[2].trainable = True
#   optimizer = Adam(
#       learning_rate=6e-06, # this learning rate is for bert model.
#       epsilon=1e-08,
#       decay=0.01,
#       clipnorm=1.0)
# 
#   # Set loss and metrics
#   loss = BinaryCrossentropy(from_logits = True)
#   metric = BinaryAccuracy('accuracy'),
#   # Compile the model
#   model.compile(
#       optimizer = optimizer,
#       loss = loss, 
#       metrics = metric)
#   final = model.fit(
#       x ={'input_ids':train_x['input_ids'],'attention_mask':train_x['attention_mask']} ,
#       y = train_y,
#       epochs=9,
#       batch_size=10
#   )
#   acc = final.history['accuracy']
#   loss = final.history['loss']
#   epochs_plot = np.arange(1, len(loss) + 1)
#   plt.clf()
#   plt.plot(epochs_plot, acc, 'r', label='Accuracy')
#   plt.plot(epochs_plot, loss, 'b:', label='Loss')
#   plt.title('VISUALIZATION OF LOSS AND ACCURACY CURVE')
#   plt.xlabel('Epochs')
#   plt.legend()
#   plt.show()
#   st.pyplot()
#   # Plot the loss and accuracy curves  
#   f = plt.figure(figsize=(20,7))
#   #Adding Subplot 1 (For Accuracy)
#   f.add_subplot(121)
#   plt.plot(final.epoch,final.history['accuracy'],label = "accuracy") # Accuracy curve 
#   plt.title("Accuracy Curve",fontsize=18)
#   plt.xlabel("Epochs",fontsize=15)
#   plt.ylabel("Accuracy",fontsize=15)
#   plt.grid(alpha=0.3)
#   plt.legend()
#   #Adding Subplot 1 (For Loss)
#   f.add_subplot(122)
#   plt.plot(final.epoch,final.history['loss'],label="loss") # Loss curve 
#   plt.title("Loss Curve",fontsize=18)
#   plt.xlabel("Epochs",fontsize=15)
#   plt.ylabel("Loss",fontsize=15)
#   plt.grid(alpha=0.3)
#   plt.legend()
#   plt.show()
#   st.pyplot()
#   test_x = tokenizer(
#     text=test_data.text.tolist(),
#     add_special_tokens=True,
#     max_length=36,
#     truncation=True,
#     padding=True, 
#     return_tensors='tf',
#     return_token_type_ids = False,
#     return_attention_mask = True,
#     verbose = True)
#   predicted = model.predict({'input_ids':test_x['input_ids'],'attention_mask':test_x['attention_mask']})
#   y_predicted = np.where(predicted>0.5,1,0)
#   y_predicted = y_predicted.reshape((1,3263))[0]
#   sample_data['id'] = test_data.id
#   sample_data['text'] = test_data.text
#   sample_data['target'] = y_predicted
#   st.dataframe(sample_data)

!streamlit run /content/Visualize_The_Data.py & npx localtunnel --port 8501